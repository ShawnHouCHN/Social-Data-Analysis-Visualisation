{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading the balanced dataframe we created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comb = pd.read_csv('NYPD_ML_df_wY_balanced.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having been included earlier - we drop some of the observations that intuitively do not make much sense for affecting whether a person is injured/killed during an accident. When applying ML it is important to keep in mind what we are doing instead of blindly training models on things that might not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comb = df_comb.drop(['Minimum Humidity', 'Maximum Humidity', 'Sea Level Pressure', 'Max Temperature', 'Min Temperature'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataframe contains a total of 207782 class balanced observations with 31 features (excluding the 'Y' column that holds class information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215862, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the features and classes into their own NumPy arrays to be fed into sklearn's and Keras's ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting X and y\n",
    "\n",
    "X_cols = list(df_comb.columns)\n",
    "X_cols.remove('Y')\n",
    "y_cols = 'Y'\n",
    "\n",
    "X, y = df_comb[X_cols].values, df_comb[y_cols].values \n",
    "y = y.astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also standardize the data as it has been shown to increase model performance in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some basic evaluation we are using hold-out cross validation with an 80-20 training-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary modelling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for how good our dataset is we briefly train and test a few models on the hold out split data above. This gives us a rough estimate of how good our dataset is and whether it can be modelled effectively. The hyperparameters chosen for the models are quite arbitrary, but later on we will optimize these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we try is Random Forest. Random forest works by creating an ensemble of decision trees that each get trained on a subset (sampled with replacement, akin to bootstrapping) of the total data. At each node of each decision tree, $m$ features are selected at random and they used to split the node samples as well as possible (measured using an impurity measure).\n",
    "\n",
    "In this test we used an ensemble of 500 decision trees, the gini impurity measure and max features as a square root of the total feature number ($\\sqrt34 \\approx 6$) (default in sklearn). Other parameters are according to the defaults in sklearn's Random Forest implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "rf_model = RandomForestClassifier(criterion='gini', max_features='auto', random_state=0, n_estimators=300, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=300, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model to training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicting test classes\n",
    "rf_y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61142380654575779"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_score(y_pred=rf_y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNearest Neighbours (KNN) is a relatively intuitive algorithm that works by looking at the distance (e.g. euclidian distance) between data points and assigning a new point to the same class as the point(s) it is closest to. It has been called a \"lazy\" learning algorithm as there is not parameter tuning involved during training, but rather simply assigning new points to their closest neighbours. \n",
    "\n",
    "There are some hyperparemters that can be chosen however, such as the distance metric and n_neighbours. In this case we are using the Minkowski distance metric with a default value of p=2 (meaning, euclidian distance) and each test observation is compared to its 5 closest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "knn_model = KNeighborsClassifier(metric='minkowski', n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model (in the case of KNN the .fit looks at the training data to determine the optimal KNN algorithm used)\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict classes\n",
    "knn_y_pred = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61123850554744863"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_score(y_pred=knn_y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVM) that try to find the optimal hyperplane to separate data of certain classes. While many ML models find a decision boundary that separates data points of different classes from each other (such as logistic regression), SVM differ in a way that they try to find the *best* decision boundary that separates observations.\n",
    "\n",
    "They do this by looking at observations in the training data belonging to different classes that are close to each other (i.e. \"difficult observations\", also called \"support vectors\"). After identifying these difficult to classify observations, the SVM attempts to find a decision boundary that maximises its distance to these difficult observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "svm_model = LinearSVC(random_state=0, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict classes\n",
    "svm_y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65283857966784797"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "accuracy_score(y_pred=svm_y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear probility model used to predict the probability a given observation belongs to a certain class. The function it uses takes the form of:\n",
    "\n",
    "$\\frac{1}{1 + e^{-t}}$\n",
    "\n",
    "Assuming that $t$ is expressed as a linear model, one can use ordinary least squares to estimate the coefficients that best fit the data. Weighting input with these coefficients and feeding them to the function above, one can interpret the result as the probability of $y = 1$ given that the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65267644129432745"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=lr_y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks are machine learning models inspired by how brains work. Loosely speaking, an artificial neural network is composed of layers that do information processing on a given input. After processing, the network gives its final output. Some of the advantages of neural networks is their ability to detect complex non linear relationships and being able to model a subset of all possible interactions in the input features.\n",
    "\n",
    "Here we define a neural network with an input dimension of 30 nodes. The network has two hidden layers with 15 and 7 nodes respectively and output node activated by the sigmoid function. All other layers are activated with the relU activation function. The loss function used is binary crossentropy and the learning is optimized with Adaptive Moment Estimation (Adam). We are running 60 epochs for the training with a batch size of 1000 (as our dataset is relatively large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "nn_model.add(Dense(15, activation='relu'))\n",
    "nn_model.add(Dense(7, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ba66550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit(X_train, y_train, nb_epoch=60, batch_size=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_y_pred = nn_model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6530007180413685"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred=nn_y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, most of the models are getting around 61-65% accuracy on the hold out test set - which is considerably better than the 50% baseline. Random Forest and KNN do the worst with around 61% while SVM, LR and NN do better with a score around 65%.\n",
    "\n",
    "Taking into consideration training time, the LR would most likely be the \"best\" model of the ones we have tested so far, as it takes the shortest amount of time to train (of all the models). The KNN would by far be the worst, as it takes the longest and gives the worst score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and accurate performance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize hyperparameters for the models shown above we apply grid search. Grid search tries all combinations of a given hyperparameter set and chooses the combination that gives the lowest test error. It is important to be vigilant when reporting test errors, as just the choice of a the \"best\" hyperparameter combination introduces a certain bias (fit to the test data). Therefore we apply a nested cross validation method, wherein the hyperparameters are optimized on an inner loop, but their evaluated on an \"unseen\" outer loop in each outer fold.\n",
    "\n",
    "For this experiment we chose 3 outer and 3 inner loops for each of the models to be tested on. In the case of the neural network we did not do any hyperparameter optimization, but rather used the same model as defined above and evaluated it on 3 outer loops after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nested_CV(model, model_param_grid, X, y, model_list, mean_rs_TE_scores, mean_rs_GE_scores, verbose):\n",
    "        \n",
    "    # note: shuffle must be true for random state to do anything\n",
    "    outer_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0).split(X, y) # split into outer folds for GE\n",
    "    inner_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0) # split into inner folds for MS\n",
    "\n",
    "    scores = [] # EVAL scores for MCC\n",
    "    scores_acc = [] # EVAL scores for Accuracy\n",
    "    \n",
    "    test_scores = [] # TEST scores for MCC\n",
    "\n",
    "    # outer fold \n",
    "    for k, (train, test) in enumerate(outer_kfold):\n",
    "         \n",
    "        # INNER LOOP FOR HYPERPARAMETERS #\n",
    "                \n",
    "        scorer = make_scorer(matthews_corrcoef) # MCC for scoring when selecting a model in the GS\n",
    "    \n",
    "        # define the GS # \n",
    "        gs = GridSearchCV(estimator=model,\n",
    "                    param_grid=model_param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=inner_kfold,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "        gs.fit(X[train], y[train]) # fit and test on the inner fold \n",
    "        \n",
    "        best_model = gs.best_estimator_ # select the best model from the inner loop\n",
    "        best_inner_score = gs.best_score_ # grab the best model's score\n",
    "        test_scores.append(best_inner_score) # add to list holding all inner scores\n",
    "        \n",
    "        # OUTER LOOP TRAINING AND PREDICTION # \n",
    "                \n",
    "        best_model.fit(X[train], y[train]) # fit training with the best model\n",
    "        y_pred = best_model.predict(X[test]) # predict with best model \n",
    "        model_list.append(best_model) # add the model selected from the inner fold\n",
    "        \n",
    "        # EVALUATION ON OUTER LOOP # \n",
    "        \n",
    "        # Confusion matrix for each outer loop used for the generalization error\n",
    "        #confmat = confusion_matrix(y_true=y[test], y_pred=y_pred)\n",
    "        \n",
    "        # evaluation with MCC and accuracy\n",
    "        score = matthews_corrcoef(y_true=y[test], y_pred=y_pred)\n",
    "        scores.append(score)\n",
    "        score_acc = accuracy_score(y_true=y[test], y_pred=y_pred)\n",
    "        scores_acc.append(score_acc)    \n",
    "                \n",
    "        if verbose == 1:\n",
    "            print('OUTER FOLD: {}'.format(k+1)) # which fold\n",
    "            print('BEST ESTIMATOR FROM INNER: {}'.format(gs.best_params_)) # best estimator\n",
    "            print('TE in FOLD: ACC = {:.3f}'.format(best_inner_score)) \n",
    "            print('GE in FOLD: MCC = {:.3f}, ACC = {:.3f}, TRAIN CLASS DIST: {}'.format(score, score_acc, np.bincount(y[train])))\n",
    "            print('\\n')\n",
    "    \n",
    "\n",
    "    # MCC mean\n",
    "    print('\\nMean CV TE MCC : %.3f +/- %.3f' % (np.mean(test_scores), np.std(test_scores)))\n",
    "    print('\\nMean CV EVAL MCC : %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
    "              \n",
    "    mean_rs_TE_scores.append(np.mean(test_scores))\n",
    "    mean_rs_GE_scores.append(np.mean(scores))\n",
    "\n",
    "    # Accuracy mean\n",
    "    print('\\nMean CV EVAL Accuracy: %.3f +/- %.3f' % (np.mean(scores_acc), np.std(scores_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER FOLD: 1\n",
      "BEST ESTIMATOR FROM INNER: {'criterion': 'gini', 'n_estimators': 300}\n",
      "TE in FOLD: ACC = 0.614\n",
      "GE in FOLD: MCC = 0.231, ACC = 0.615, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 2\n",
      "BEST ESTIMATOR FROM INNER: {'criterion': 'gini', 'n_estimators': 300}\n",
      "TE in FOLD: ACC = 0.620\n",
      "GE in FOLD: MCC = 0.225, ACC = 0.612, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 3\n",
      "BEST ESTIMATOR FROM INNER: {'criterion': 'gini', 'n_estimators': 300}\n",
      "TE in FOLD: ACC = 0.618\n",
      "GE in FOLD: MCC = 0.230, ACC = 0.615, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "\n",
      "Mean CV TE MCC : 0.617 +/- 0.002\n",
      "\n",
      "Mean CV EVAL MCC : 0.229 +/- 0.003\n",
      "\n",
      "Mean CV EVAL Accuracy: 0.614 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "# Optimizing and testing random forest\n",
    "(rf_model_list, rf_TE_scores, rf_GE_scores) = ([], [], [])\n",
    "rf_model = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rf_param_grid = [{'n_estimators': [50, 150, 300], 'criterion': ['gini']}]\n",
    "\n",
    "nested_CV(rf_model, rf_param_grid, X, y, rf_model_list, rf_TE_scores, rf_GE_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizing and testing KNearest neighbours\n",
    "(knn_model_list, knn_TE_scores, knn_GE_scores) = ([], [], [])\n",
    "knn_model = KNeighborsClassifier(metric='minkowski')\n",
    "knn_param_grid= [{'n_neighbors': [3, 4, 5], 'p': [1, 2]}]\n",
    "\n",
    "nested_CV(knn_model, knn_param_grid, X, y, knn_model_list, knn_TE_scores, knn_GE_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER FOLD: 1\n",
      "BEST ESTIMATOR FROM INNER: {'C': 1}\n",
      "TE in FOLD: ACC = 0.650\n",
      "GE in FOLD: MCC = 0.312, ACC = 0.651, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 2\n",
      "BEST ESTIMATOR FROM INNER: {'C': 0.1}\n",
      "TE in FOLD: ACC = 0.651\n",
      "GE in FOLD: MCC = 0.309, ACC = 0.650, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 3\n",
      "BEST ESTIMATOR FROM INNER: {'C': 0.1}\n",
      "TE in FOLD: ACC = 0.650\n",
      "GE in FOLD: MCC = 0.312, ACC = 0.651, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "\n",
      "Mean CV TE MCC : 0.651 +/- 0.000\n",
      "\n",
      "Mean CV EVAL MCC : 0.311 +/- 0.001\n",
      "\n",
      "Mean CV EVAL Accuracy: 0.651 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "# Optimizing and testing support vector machine \n",
    "(svm_model_list, svm_TE_scores, svm_GE_scores) = ([], [], [])\n",
    "svm_model = LinearSVC(random_state=0)\n",
    "svm_param_grid=[{'C': [0.01, 0.1, 1]}]\n",
    "\n",
    "nested_CV(svm_model, svm_param_grid, X, y, svm_model_list, svm_TE_scores, svm_GE_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER FOLD: 1\n",
      "BEST ESTIMATOR FROM INNER: {'C': 0.01}\n",
      "TE in FOLD: ACC = 0.650\n",
      "GE in FOLD: MCC = 0.312, ACC = 0.651, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 2\n",
      "BEST ESTIMATOR FROM INNER: {'C': 0.01}\n",
      "TE in FOLD: ACC = 0.651\n",
      "GE in FOLD: MCC = 0.308, ACC = 0.649, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 3\n",
      "BEST ESTIMATOR FROM INNER: {'C': 0.01}\n",
      "TE in FOLD: ACC = 0.650\n",
      "GE in FOLD: MCC = 0.311, ACC = 0.651, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "\n",
      "Mean CV TE MCC : 0.650 +/- 0.000\n",
      "\n",
      "Mean CV EVAL MCC : 0.310 +/- 0.001\n",
      "\n",
      "Mean CV EVAL Accuracy: 0.650 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "# Optimizing and testing logistic regression\n",
    "(lr_model_list, lr_TE_scores, lr_GE_scores) = ([], [], [])\n",
    "lr_model = LogisticRegression(random_state=0)\n",
    "lr_param_grid=[{'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "nested_CV(lr_model, lr_param_grid, X, y, lr_model_list, lr_TE_scores, lr_GE_scores, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network - nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizing and testing neural network\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "nn_model.add(Dense(15, activation='relu'))\n",
    "nn_model.add(Dense(7, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER FOLD: 1\n",
      "GE in FOLD: MCC = 0.308, ACC = 0.653, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 2\n",
      "GE in FOLD: MCC = 0.308, ACC = 0.653, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "OUTER FOLD: 3\n",
      "GE in FOLD: MCC = 0.312, ACC = 0.655, TRAIN CLASS DIST: [71954 71954]\n",
      "\n",
      "\n",
      "\n",
      "Mean CV EVAL MCC : 0.309 +/- 0.002\n",
      "\n",
      "Mean CV EVAL Accuracy: 0.654 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "outer_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0).split(X, y) \n",
    "\n",
    "(scores, scores_acc, test_scores) = ([], [], [])\n",
    "\n",
    "for k, (train, test) in enumerate(outer_kfold):\n",
    "    \n",
    "    nn_model.fit(X[train], y[train], nb_epoch=60, batch_size=1000, verbose=0)\n",
    "    \n",
    "    y_pred = nn_model.predict_classes(X[test], verbose=0)\n",
    "    \n",
    "    score = matthews_corrcoef(y_true=y[test], y_pred=y_pred)\n",
    "    scores.append(score)\n",
    "    score_acc = accuracy_score(y_true=y[test], y_pred=y_pred)\n",
    "    scores_acc.append(score_acc)\n",
    "    \n",
    "    print('OUTER FOLD: {}'.format(k+1)) # which fold\n",
    "    print('GE in FOLD: MCC = {:.3f}, ACC = {:.3f}, TRAIN CLASS DIST: {}'.format(score, score_acc, np.bincount(y[train])))\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "# MCC mean\n",
    "print('\\nMean CV EVAL MCC : %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
    "print('\\nMean CV EVAL Accuracy: %.3f +/- %.3f' % (np.mean(scores_acc), np.std(scores_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above results - the performance of the models does not change by a large margin after applying the nexted cross validation. This is most likely due to the fact we have a relatively large dataset, and a 20% holdout evaluation might be enough to provide an accurate performance measure. Additionally, in some cases the best models from the inner loop can differ in their hyperparameters. It can be that due to the large size of the dataset there is little difference in choice of hyperparameters (they can all fit the data pretty much equally well). If a final classifier was being made, a good choice might be to have an ensemble of classifiers (whether it be same kind of model with different hyperparameter combinations or even different models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at feature importance w. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting application of Random Forests is to calculate feature importance of input features. Briefly it works by taking a part of the samples (that were not used to train a given tree, a part is held out in the Random Forests implementation), running them trough the decision trees and calcultating the classification score. Next, a given feature is selected and it randomly permutated in the samples not used for training. These samples are again run through the decision trees and the accuracy calculated. The difference in accuracy with and without the random permutations of a given feature is used to calculate its importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(criterion='gini', random_state=0, n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 0 TIME 25.09\n",
      "2. feature 1 Average Humidity 9.20\n",
      "3. feature 4 Mean Temperature 8.25\n",
      "4. feature 2 Max Gust Speed 7.42\n",
      "5. feature 9 Wind Speed 6.32\n",
      "6. feature 3 Max Wind Speed 5.63\n",
      "7. feature 11 Month 5.26\n",
      "8. feature 26 VTC2_two_wheeler 4.74\n",
      "9. feature 8 Visibility 3.10\n",
      "10. feature 24 VTC2_other 3.08\n",
      "11. feature 5 Precipitation 2.91\n",
      "12. feature 10 street_SL 2.74\n",
      "13. feature 25 VTC2_small 2.08\n",
      "14. feature 23 VTC2_medium 1.93\n",
      "15. feature 14 MANHATTAN 1.69\n",
      "16. feature 13 BROOKLYN 1.31\n",
      "17. feature 15 QUEENS 1.29\n",
      "18. feature 12 BRONX 1.07\n",
      "19. feature 20 VTC1_small 1.06\n",
      "20. feature 18 VTC1_medium 1.03\n",
      "21. feature 7 Snow Depth 0.90\n",
      "22. feature 21 VTC1_two_wheeler 0.62\n",
      "23. feature 27 Rain_EV 0.62\n",
      "24. feature 22 VTC2_large 0.58\n",
      "25. feature 16 STATEN ISLAND 0.56\n",
      "26. feature 19 VTC1_other 0.44\n",
      "27. feature 29 Fog_EV 0.35\n",
      "28. feature 17 VTC1_large 0.34\n",
      "29. feature 6 Snow 0.29\n",
      "30. feature 28 Snow_EV 0.12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAAF1CAYAAAAtPu9jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4bXdZH/DvSwIoYYiQMCWBpG2kUhWUa6BtxIMUTOIQ\nah/bWNTi0DQtEWNrLWrLg1WfBxVr7VMkjZA6MASVwWijDLVH2yKYG5pAEgYvIfTeMOQiUBEsIeTt\nH3tdu3M4w773rH3Ouet+Ps9znrP32mv93t9avz2c71nDru4OAADAFN1ntzsAAACwLAIPAAAwWQIP\nAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPADuuqq6sqn+z2/0AYPrK9/AAHD+q6vYkj0jy+bnJX9rd\nH9pGmytJXtHdZ26vd8enqvrlJIe6+1/vdl8AGJ89PADHn2/u7gfO/Rxz2BlDVZ28m/W3o6pO2u0+\nALBcAg/ARFTVU6rqrVX1yaq6adhzc+Sx766qd1fVp6rqtqr6J8P0U5L8bpJHV9WfDz+Prqpfrqqf\nnFt+paoOzd2/var+VVW9M8mnq+rkYbnXVtXhqvpAVT1vk77+ZftH2q6qH66qO6vqw1X1rKq6qKre\nV1Ufr6ofnVv2hVX1m1X1mmF93lFVT5h7/MuqanXYDrdU1besqfvSqrquqj6d5HuTPDvJDw/r/tvD\nfM+vqvcP7d9aVX93ro3nVNX/qKoXV9UnhnW9cO7xh1bVf66qDw2Pv2HusW+qqhuHvr21qr5y7rF/\nVVV3DDXfW1VPX2DYAdiCwAMwAVV1RpL/kuQnkzw0yQ8leW1VnT7McmeSb0ry4CTfneTnq+qru/vT\nSS5M8qFj2GP07Um+McmpSe5J8ttJbkpyRpKnJ7miqr5hwbYemeSLhmVfkOSXknxHkicl+dok/6aq\nzpmb/+IkvzGs66uSvKGq7ltV9x368aYkD0/y/UleWVWPm1v2Hyb5qSQPSvKrSV6Z5GeGdf/mYZ73\nD3UfkuTHk7yiqh4118aTk7w3yWlJfibJy6uqhsd+LckDkvyNoQ8/nyRV9VVJrk7yT5I8LMl/SnJt\nVd1/6N/lSb6mux+U5BuS3L7gtgNgEwIPwPHnDcMegk/O7T34jiTXdfd13X1Pd785yf4kFyVJd/+X\n7n5/z/xBZoHga7fZj//Q3Qe7+y+SfE2S07v733b3Xd19W2ah5ZIF2/pckp/q7s8luSazIPEL3f2p\n7r4lya1JnjA3/w3d/ZvD/P8us7D0lOHngUleNPTj95P8Tmbh7Ijf6u7/OWyn/7teZ7r7N7r7Q8M8\nr0nyJ0nOm5vlg939S939+SS/kuRRSR4xhKILk1zW3Z/o7s8N2ztJLk3yn7r77d39+e7+lSSfHfr8\n+ST3T/L4qrpvd9/e3e9fcNsBsAmBB+D486zuPnX4edYw7bFJvm0uCH0yyfmZ/SGeqrqwqt42HB72\nycyC0Gnb7MfBuduPzeywuPn6P5rZBRYW8adDeEiSvxh+f3Tu8b/ILMh8Qe3uvifJoSSPHn4ODtOO\n+GBme47W6/e6quq75g49+2SSL8+9t9dH5up/Zrj5wCRnJfl4d39inWYfm+RfrNlGZyV5dHcfSHJF\nkhcmubOqrqmqR2/VTwC2JvAATMPBJL82F4RO7e5TuvtFVXX/JK9N8uIkj+juU5Ncl+TIIVjrXa7z\n05kdlnXEI9eZZ365g0k+sKb+g7r7om2v2frOOnKjqu6T5MwkHxp+zhqmHfGYJHds0O8vuF9Vj81s\n79TlSR42bK+b8/+312YOJnloVZ26wWM/tWYbPaC7X50k3f2q7j4/s2DUSX56gXoAbEHgAZiGVyT5\n5qr6hqo6qaq+aLgYwJlJ7pfZ4VKHk9w9nGD/zLllP5rkYVX1kLlpNya5aDgB/5GZ7X3YzB8n+dRw\n4v0XD3348qr6mtHW8N6eVFXfWrMrxF2R2aFhb0vy9iSfyewiBPet2YUbvjmzw+Q28tEkf2Xu/imZ\nBY7DyeyCD5nt4dlSd384s4tA/GJVfcnQh6cOD/9Sksuq6sk1c0pVfWNVPaiqHldVXz+E0/+b2R6t\nezYoA8BREHgAJqC7D2Z2Iv+PZvaH+sEk/zLJfbr7U0mel+TXk3wis5P2r51b9j1JXp3ktuFQq0dn\nduL9TZmdOP+mJK/Zov7nM7sowhOTfCDJx5K8LLOT/pfht5L8g8zW5zuTfOtwvsxdmQWcC4c+/GKS\n7xrWcSMvz+zcmU9W1Ru6+9YkP5fkjzILQ1+R5H8eRd++M7Nzkt6T2cUirkiS7t6f5B8n+Y9Dvw8k\nec6wzP2TvGjo80cyu9jBjxxFTQA24ItHATiuVNULk/y17v6O3e4LAHufPTwAAMBkCTwAAMBkOaQN\nAACYLHt4AACAyRJ4AACAyTp5tzuwntNOO63PPvvs3e4GAACwR91www0f6+7Tt5pvTwaes88+O/v3\n79/tbgAAAHtUVX1wkfkc0gYAAEyWwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEyW\nwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEyWwAMAAEzWCRl4VlZWsrKystvdAAAAluyEDDwA\nAMCJQeABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAm\nS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+AB\nAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAmS+ABAAAm\na6HAU1UXVNV7q+pAVT1/ncefXVXvrKp3VdVbq+oJc4/dPky/sar2j9l5AACAzZy81QxVdVKSlyR5\nRpJDSa6vqmu7+9a52T6Q5Ou6+xNVdWGSq5I8ee7xp3X3x0bsNwAAwJa2DDxJzktyoLtvS5KquibJ\nxUn+MvB091vn5n9bkjPH7ORRqRp33u5j7wsAALCrFjmk7YwkB+fuHxqmbeR7k/zu3P1O8paquqGq\nLt1ooaq6tKr2V9X+w4cPL9AtAACAzS2yh2dhVfW0zALP+XOTz+/uO6rq4UneXFXv6e4/XLtsd1+V\n2aFw2bdvn90qAADAti2yh+eOJGfN3T9zmHYvVfWVSV6W5OLu/tMj07v7juH3nUlen9khcgAAAEu3\nSOC5Psm5VXVOVd0vySVJrp2foaoek+R1Sb6zu983N/2UqnrQkdtJnpnk5rE6DwAAsJktD2nr7rur\n6vIkb0xyUpKru/uWqrpsePzKJC9I8rAkv1izCwHc3d37kjwiyeuHaScneVV3/95S1gQAAGCNhc7h\n6e7rkly3ZtqVc7e/L8n3rbPcbUmesHY6AADATljoi0cBAACORwIPAAAwWQIPAAAwWQIPAAAwWQIP\nAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAw\nWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIPAAAwWQIP\nAAAwWQIPAAAwWQIPAAAwWSfvdgd2w+pudwAAANgR9vAAAACTJfAAAACTJfAAAACTJfAAAACTJfAA\nAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACT\nJfAAAACTJfAAAACTJfAAAACTJfAAAACTJfAAAACTtVDgqaoLquq9VXWgqp6/zuPPrqp3VtW7quqt\nVfWERZcFAABYli0DT1WdlOQlSS5M8vgk315Vj18z2weSfF13f0WSn0hy1VEsCwAAsBSL7OE5L8mB\n7r6tu+9Kck2Si+dn6O63dvcnhrtvS3LmossCAAAsyyKB54wkB+fuHxqmbeR7k/zuMS4LAAAwmpPH\nbKyqnpZZ4Dn/GJa9NMmlSfKYxzxmzG4BAAAnqEX28NyR5Ky5+2cO0+6lqr4yycuSXNzdf3o0yyZJ\nd1/V3fu6e9/pp5++SN8BAAA2tUjguT7JuVV1TlXdL8klSa6dn6GqHpPkdUm+s7vfdzTLAgAALMuW\nh7R1991VdXmSNyY5KcnV3X1LVV02PH5lkhckeViSX6yqJLl72Fuz7rJLWhcAAIB7qe7e7T58gX37\n9vX+/fuPbeFZ4BrPHtw+AABwoquqG7p731bzLfTFowAAAMcjgQcAAJgsgQcAAJgsgQcAAJgsgQcA\nAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgs\ngQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcA\nAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgs\ngQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcA\nAJgsgQcAAJgsgQcAAJgsgQcAAJgsgQcAAJishQJPVV1QVe+tqgNV9fx1Hv/rVfVHVfXZqvqhNY/d\nXlXvqqobq2r/WB0HAADYyslbzVBVJyV5SZJnJDmU5Pqqura7b52b7eNJnpfkWRs087Tu/th2OwsA\nAHA0FtnDc16SA919W3ffleSaJBfPz9Ddd3b39Uk+t4Q+AgAAHJNFAs8ZSQ7O3T80TFtUJ3lLVd1Q\nVZduNFNVXVpV+6tq/+HDh4+ieQAAgPXtxEULzu/uJya5MMlzq+qp683U3Vd1977u3nf66afvQLcA\nAICpWyTw3JHkrLn7Zw7TFtLddwy/70zy+swOkQMAAFi6RQLP9UnOrapzqup+SS5Jcu0ijVfVKVX1\noCO3kzwzyc3H2lkAAICjseVV2rr77qq6PMkbk5yU5OruvqWqLhsev7KqHplkf5IHJ7mnqq5I8vgk\npyV5fVUdqfWq7v695awKAADAvW0ZeJKku69Lct2aaVfO3f5IZoe6rfVnSZ6wnQ4CAAAcq524aAEA\nAMCuEHgAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJ\nEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngA\nAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJ\nEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngA\nAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJEngAAIDJWijwVNUFVfXeqjpQ\nVc9f5/G/XlV/VFWfraofOpplAQAAlmXLwFNVJyV5SZILkzw+ybdX1ePXzPbxJM9L8uJjWBYAAGAp\nFtnDc16SA919W3ffleSaJBfPz9Ddd3b39Uk+d7TLAgAALMsigeeMJAfn7h8api1iO8sCAABsy565\naEFVXVpV+6tq/+HDh3e7OwAAwAQsEnjuSHLW3P0zh2mLWHjZ7r6qu/d1977TTz99weYBAAA2tkjg\nuT7JuVV1TlXdL8klSa5dsP3tLAsAALAtJ281Q3ffXVWXJ3ljkpOSXN3dt1TVZcPjV1bVI5PsT/Lg\nJPdU1RVJHt/df7besstamb1kZWUlSbK6urqr/QAAgBPZloEnSbr7uiTXrZl25dztj2R2uNpCywIA\nAOyEPXPRAgAAgLEJPAAAwGQJPAAAwGQJPAAAwGQtdNEC1qgad97uY+8LAACwIXt4AACAyRJ4AACA\nyRJ4AACAyRJ4AACAyXLRgiVZ3YEaKysrs1qrO1ENAACOP/bwAAAAkyXwAAAAkyXwAAAAkyXwAAAA\nk+WiBXtV1bjzdh97XwAA4DhlDw8AADBZAg8AADBZAg8AADBZAg8AADBZAg+bWllZycrKym53AwAA\njomrtB3HVne7AwAAsMfZwwMAAEyWwAMAAEyWQ9pOZL7cFACAibOHBwAAmCyBBwAAmCyBBwAAmCyB\nBwAAmCwXLWBTq7vdAQAA2AZ7eAAAgMkSeAAAgMkSeNgTVlZWsrKystvdAABgYgQeAABgsgQeOM7Y\nGwYAsDiBBwAAmCyBBwAAmCzfw8NyVY0/f/ex9QUAgBOOPTwAAMBkCTwAAMBkCTycMFzdDADgxOMc\nHvaE1d3uAAAAk2QPDwAAMFkCD4zIYXMAAHuLQ9qYhqO5/LVLXwMAnDDs4QEAACZL4AEAACZL4AEA\nACZroXN4quqCJL+Q5KQkL+vuF615vIbHL0rymSTP6e53DI/dnuRTST6f5O7u3jda7+EorG63AecJ\nAQAcd7YMPFV1UpKXJHlGkkNJrq+qa7v71rnZLkxy7vDz5CQvHX4f8bTu/thovQYAAFjAIoe0nZfk\nQHff1t13JbkmycVr5rk4ya/2zNuSnFpVjxq5rwAAAEdlkcBzRpKDc/cPDdMWnaeTvKWqbqiqSzcq\nUlWXVtX+qtp/+PDhBboFAACwuZ24aMH53f3EzA57e25VPXW9mbr7qu7e1937Tj/99B3oFgAAMHWL\nBJ47kpw1d//MYdpC83T3kd93Jnl9ZofIAQAALN0iV2m7Psm5VXVOZiHmkiT/cM081ya5vKquyexi\nBf+nuz9cVackuU93f2q4/cwk/3a87sPesrrdBlwJDgBgVFsGnu6+u6ouT/LGzC5LfXV331JVlw2P\nX5nkuswuSX0gs8tSf/ew+COSvH521eqcnORV3f17o68FAADAOhb6Hp7uvi6zUDM/7cq5253kuess\nd1uSJ2yzj8BEraysJElWV1d3tR8AwHTtxEULAAAAdoXAAwAATJbAAwAATJbAAwAATJbAAwAATJbA\nAwAATJbAAwAATJbAAwAATJbAAwAATJbAAwAATJbAA3AcWFlZycrKym53AwCOOwIPAAAwWQIPAAAw\nWQIP8AUcPnViMu4ATJHAAwAATNbJu90BYIdVjTtv97H3ZQcc2WOxurp6XNcAAI6NwAOM72hC1aLz\n7/FgBQDsTQIPHGdWd7sDe8kJtrcKADh6Ag/wBVZ3uwMAACMReAA2Yy8SABzXBB5g0lZ3uwMAwK5y\nWWoAAGCyBB4AAGCyHNIGsE2ru90BAGBD9vAAAACTJfAAsKNWVlaysrKy290A4AQh8AAAAJPlHB6A\n3ea7fgBgaQQegBOBUAXACcohbQAAwGQJPABMjgsjAHCEQ9oAGMfRHDa36Px7+NC5I4FqdXV1V/sB\nwObs4QEAACbLHh5g16zudgcAgMmzhwcA9qidOBfJ+U7A1NnDA8DxxSW2j0vOeQJ2i8ADAGsJVQCT\n4ZA2AABgsuzhAWBHre52B/aKndiLtNdqbKcOo3KIIScSe3gAgElwAQZgPfbwAMAxWN3tDpxonFcF\nHCOBB4DJWd3tDrArVne7A8CeJPAAwB61OpEaO1ln2Xbi3JcpnV8zpXXh+CXwAAAke+8iD3u5xh4i\nVLEVgQfgOLA6kRpwvFudSI1tc3U+jiMCDwAAS7G63Qb2yN4qhzIe3xYKPFV1QZJfSHJSkpd194vW\nPF7D4xcl+UyS53T3OxZZFgAAjtXqdhtYICj9ZY3j/DDDEzVUbRl4quqkJC9J8owkh5JcX1XXdvet\nc7NdmOTc4efJSV6a5MkLLgsAACe01e02sBPB7Ti1yBePnpfkQHff1t13JbkmycVr5rk4ya/2zNuS\nnFpVj1pwWQAAYCL22pcAL3JI2xlJDs7dP5TZXpyt5jljwWUBANhBq7vdAY5PCx6at3o08+/AnqQ9\nc9GCqro0yaXD3T+vqvcuueRpST625VxHexUSNfZ6HTX2Vo2dqqPG3qqxU3XU2Fs1dqqOGnurxk7V\nUWNv1dipOo9dZKZFAs8dSc6au3/mMG2Ree67wLJJku6+KslVC/RnFFW1v7v3qbE3auxUHTX2Vo2d\nqqPG3qqxU3XU2Fs1dqqOGnurxk7VUWNv1djJOotY5Bye65OcW1XnVNX9klyS5No181yb5Ltq5ilJ\n/k93f3jBZQEAAJZiyz083X13VV2e5I2ZXVr66u6+paouGx6/Msl1mV2S+kBml6X+7s2WXcqaAAAA\nrLHQOTzdfV1moWZ+2pVztzvJcxdddo/YicPn1Nh7ddTYWzV2qo4ae6vGTtVRY2/V2Kk6auytGjtV\nR429VWMn62ypekLX2AYAAJi3yDk8AAAAx6UTLvBU1QVV9d6qOlBVz19Sjaur6s6qunkZ7c/VOamq\n/ldV/c6S2j+rqv5bVd1aVbdU1Q8sqc4PVNXNQ40rltD+F1XVH1fVTUONHx+x7S8Y66r6tqHOPVW1\n7auTbDYOVfX9VfWeYfrPbLfWXLs/OLR5c1W9uqq+aIQ2N30+VdW/qKquqtO2W2uuzdur6l1VdWNV\n7R+x3fXG/aFV9eaq+pPh95dss8a626uqXlhVdwzrdGNVXbSEGj9RVe8c2n9TVT16GzU2fD8cc8w3\nGJPR1mOTGqONxyY1fnZ4nb+zql5fVadus8ZG4z7ac3iD9XhiVb3tyOuxqs7bznqsqfe4uTG4sar+\nbIzPk0221Y68z1fVa+bW6faqunEJNUZ7fm30eq+RP6s2eH49oar+aHjP/+2qevA22t9oW41WY5P1\nGG3MN6h5alX95jAe766qvzlCmxttr6W95o9ad58wP5ldOOH9Sf5KkvsluSnJ45dQ56lJvjrJzUte\nn3+e5FVJfmdJ7T8qyVcPtx+U5H1jb68kX57k5iQPyOycsrck+Wsj16gkDxxu3zfJ25M8ZVljneTL\nkjwus+/d2rescUjytGF73X947OEjrdMZST6Q5IuH+7+e5DnLfD5ldvn6Nyb5YJLTRhz728dsb4tx\n/5kkzx9uPz/JTy9p3F+Y5IdGWo+Najx4bp7nJblyzG21jDHfYExGW49Naow2HpvUeGaSk4fbP73E\n59Zoz+EN1uNNSS4cbl+UZHWs7bam9klJPpLksSO0tdG22pH3+TXz/FySFyxhXUZ7fm0w7qN/Vm1Q\n5/okXzfc/p4kP7GEbTVajY3WY8wx36DNX0nyfcPt+yU5dYQ2N9peO/KaX+TnRNvDc16SA919W3ff\nleSaJBePXaS7/zDJx8dud15VnZnkG5O8bFk1uvvD3f2O4fankrw7sz+Gx/RlSd7e3Z/p7ruT/EGS\nbx2zQM/8+XD3vsPPKCevrTfW3f3u7h7ti3M3GYd/muRF3f3Z4bE7x6qZWfj84qo6ObMw+qHtNrjF\n8+nnk/xwRhqXZdvgNX5xZh8kGX4/a5s1lv7626hGd//Z3GynZBvjssn74ahjvsFrcbT12KjG2DZY\njzcN749J8rbMvtNuOzU2em6N9hzeYFt1kiP/EX9IRnhf2cDTk7y/uz+43YY2eY3s1Pt8kqSqKsnf\nT/LqsWuM+fzaYNxH/6zaoM6XJvnD4fabk/y9bbS/0XiMVmNoe8P3lDHGfJ02H5JZyHr5UP+u7v7k\ndtvdZHvt1Gt+Syda4DkjycG5+4cy/h/wO+XfZ/bHwj07Uayqzk7yVZntHRnTzUm+tqoeVlUPyOw/\nAGdtscxRq9nhfzcmuTPJm7t77PXYEWvG4Usz23Zvr6o/qKqvGaNGd9+R5MVJ/neSD2f2vVpvGqPt\nI+bXo6ouTnJHd980Zo1BJ3lLVd1QVZcuof15j+jZ948ls/8uP2Kshtd5/X3/cPjJ1ds57GizGlX1\nU1V1MMmzk7xgjBpztZY55mtrLW095ow+Hpv4niS/O1Zja8Z9ac/hwRVJfnYYjxcn+ZGR2z/ikoz4\nR+IRS/wcXKTO1yb5aHf/yRJrJCM/vwZL+axaxy35///E/raM9LfEmm21lBobGHXMB+ckOZzkP9fs\nlIiXVdUpI7a/dnvt1Gt+Syda4JmEqvqmJHd29w07VO+BSV6b5Io1/zHdtu5+d2a70N+U5PeS3Jjk\n82PWGOp8vrufmNl/rs6rqi8fu8ayrTMOJyd5aJKnJPmXSX59+I/Qdut8SWZv6OckeXSSU6rqO7bb\n7lz7f7keSe5O8qNZ3h+i5w/jfmGS51bVU5dU5156tv9+lD0X64z7SzM7LPeJmQXSn1tCjXT3j3X3\nWUlemeTy7daYq/WALHfM72VZ6zFn9PHYSFX9WGavmVeO1N6G7+1jPofn/NMkPziMxw9m+C/zmGr2\nJeffkuQ3Rm53aZ+DC9b59owU4jaqMfbza85SPqvW8T1J/llV3ZDZYVV3bbfBdbbV6DU2MdqYzzk5\ns0PoXtrdX5Xk05kdvjqKdbbX0l/zizrRAs8duXcaP3OYdrz520m+papuz+ywvK+vqlcso1BV3Tez\nJ+8ru/t1y6jR3S/v7id191OTfCKzYz+XYth1+9+SXLCsGsuwwTgcSvK64ZC9P85sb98YJ/z/nSQf\n6O7D3f25JK9L8rdGaHe99firmQWrm4bn85lJ3lFVjxyj3rC36sghFK/P7LDWZfloVT0qSYbf2z5s\nY71x7+6PDgH+niS/lG2u0wKv8Vdmm4dtrLHUMd/E2OuRZPzx2EhVPSfJNyV59hBGttveeuM++nN4\njX+U2ftJMgsky9hWFyZ5R3d/dKwGd+JzcLM6w6HF35rkNUus8ZyM+PxaY1mfVffS3e/p7md295My\nCwrv3057G7z/jlpjk9qjjfkah5IcmjvK5TczC0DbtsFzayde8ws50QLP9UnOrapzhv8CXZLk2l3u\n01Hr7h/p7jO7++zM1uH3u3u0/8AfMfwH5uVJ3t3d/27s9ufqPHz4/ZjMXuCvGrn902u46kxVfXGS\nZyR5z5g1lmmTcXhDZieDpqq+NLOTDz82Qsn/neQpVfWAofbTMzsed1vWW4/ufld3P7y7zx6ez4cy\nO/HxIyPUO6WqHnTkdmYn5i7zyonXZvbmnuH3b22nsY3G/cgfpIO/m22s0yY1zp2b7eKM+HpZ5piv\ntcz1mKteKBE4AAAB5klEQVQx2nhsUuOCzA5h/pbu/swI7W30njLqc3gdH0rydcPtr08y5qE6R4z6\nX/Ed/BzcrM7fSfKe7j60jBpjP7/WsazPqnuZ+1viPkn+dZIrt9HWRttqtBpbGGXM1xreZw9W1eOG\nSU9Pcut2293k+bsTr/nF9C5dLWG3fjI7R+R9maXyH1tSjVdndmjD5zL7MP/eJa7PSpZ3lbbzMzuk\n4Z2ZHWp2Y5KLllDnv2f2grspydOX0P5XJvlfw3rcnBGveLLeWGf2R8+hJJ9N8tEkb1zGOGT2ofGK\nYZ3ekeTrR1yvH8/sj8Obk/xahqvrLPv5lBGvqpbZYUY3DT+3jPl632DcH5bkv2b2hv6WJA9d0rj/\nWpJ3DdOvTfKoJdR47TD270zy25md2DzatlrGmG8wJqOtxyY1RhuPTWocyOz80yNjtN2rzW007qM9\nhzdYj/OT3DC8Jt+e5EnbHfc1NU9J8qdJHjJimxttqx15nx8e++Ukly1xXUZ7fm0w7qN/Vm1Q5wcy\n+/vufUlelKSWsK1Gq7HReow55hvUfGKS/cO6vSHJlyzxubXU1/zR/NTQUQAAgMk50Q5pAwAATiAC\nDwAAMFkCDwAAMFkCDwAAMFkCDwAAMFkCDwAAMFkCDwAAMFkCDwAAMFn/D+Xlt416Tzz/AAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d6cb588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"{}. feature {} {} {:.2f}\".format(f + 1, indices[f], df_comb.columns[indices[f]], importances[indices[f]]*100))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: TIME\n",
      "CAS Mean: 13.704 +/- 5.796 \t Non-CAS Mean: 13.421 +/- 5.413\n",
      "CAS Median: 15.000 \t\t Non-CAS Median: 14.000\n",
      "\n",
      "Feature: Average Humidity\n",
      "CAS Mean: 60.072 +/- 14.033 \t Non-CAS Mean: 59.619 +/- 13.999\n",
      "CAS Median: 59.000 \t\t Non-CAS Median: 58.000\n",
      "\n",
      "Feature: Mean Temperature\n",
      "CAS Mean: 14.332 +/- 9.465 \t Non-CAS Mean: 13.698 +/- 9.824\n",
      "CAS Median: 16.000 \t\t Non-CAS Median: 14.000\n",
      "\n",
      "Feature: Max Gust Speed\n",
      "CAS Mean: 35.947 +/- 10.944 \t Non-CAS Mean: 36.298 +/- 11.014\n",
      "CAS Median: 34.000 \t\t Non-CAS Median: 35.000\n",
      "\n",
      "Feature: Wind Speed\n",
      "CAS Mean: 8.557 +/- 5.743 \t Non-CAS Mean: 8.712 +/- 5.785\n",
      "CAS Median: 8.000 \t\t Non-CAS Median: 8.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for important_index in indices[0:5]:\n",
    "    \n",
    "    # 1:-1 because the first and last columns are not in X\n",
    "    column_name = X_cols[important_index]\n",
    "    cas_data = df_comb.loc[df_comb['Y'] == 1][column_name]\n",
    "    non_cas_data = df_comb.loc[df_comb['Y'] == 0][column_name]\n",
    "    \n",
    "    cas_mean, cas_std = np.mean(cas_data), np.std(cas_data)\n",
    "    non_cas_mean, non_cas_std = np.mean(non_cas_data), np.std(non_cas_data)\n",
    "    \n",
    "    cas_median, non_cas_median = np.median(cas_data), np.median(non_cas_data)\n",
    "    \n",
    "    print(\"Feature: {}\".format(column_name))\n",
    "    print(\"CAS Mean: {:.3f} +/- {:.3f} \\t Non-CAS Mean: {:.3f} +/- {:.3f}\".format(cas_mean, cas_std, non_cas_mean,non_cas_std))\n",
    "    print(\"CAS Median: {:.3f} \\t\\t Non-CAS Median: {:.3f}\\n\".format(cas_median, non_cas_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, time of collision seems to be an important feature in determining whether there is a casualty. Other interesting observations include average humidity, mean temperature, max gust speed and wind speed. Intuitively, it might not make much sense for average humidity to affect whether a person gets injured/killed or not in a collision or not. However it is important to consider that even though there might not be a direct causal relationship between a feature and an output, they can be correlated due to some underlying not-so-obvious relationship. For example, it could be that humidity is correlated with a season / other changes in weather that actually might have a causal relationship with whether a casualty occurs given an accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it can give an as good or even a better performance to model with only the important features (as some of them might be noise that contribute little or even \"confuse\" the classification). Here we try retraining the random forest with only the 15 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIME</th>\n",
       "      <th>Average Humidity</th>\n",
       "      <th>Mean Temperature</th>\n",
       "      <th>Max Gust Speed</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Max Wind Speed</th>\n",
       "      <th>Month</th>\n",
       "      <th>VTC2_two_wheeler</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>VTC2_other</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>street_SL</th>\n",
       "      <th>VTC2_small</th>\n",
       "      <th>VTC2_medium</th>\n",
       "      <th>MANHATTAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.29</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.29</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.29</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.29</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>95</td>\n",
       "      <td>7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.29</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TIME  Average Humidity  Mean Temperature  Max Gust Speed  Wind Speed  \\\n",
       "0    10                95                 7            34.0         9.0   \n",
       "1    10                95                 7            34.0         9.0   \n",
       "2    10                95                 7            34.0         9.0   \n",
       "3    11                95                 7            34.0         9.0   \n",
       "4    11                95                 7            34.0         9.0   \n",
       "\n",
       "   Max Wind Speed  Month  VTC2_two_wheeler  Visibility  VTC2_other  \\\n",
       "0            23.0      3                 0         7.0           1   \n",
       "1            23.0      3                 0         7.0           0   \n",
       "2            23.0      3                 0         7.0           0   \n",
       "3            23.0      3                 0         7.0           1   \n",
       "4            23.0      3                 0         7.0           1   \n",
       "\n",
       "   Precipitation  street_SL  VTC2_small  VTC2_medium  MANHATTAN  \n",
       "0          18.29       25.0           0            0          0  \n",
       "1          18.29       25.0           0            1          0  \n",
       "2          18.29       25.0           1            0          0  \n",
       "3          18.29       25.0           0            0          1  \n",
       "4          18.29       35.0           0            0          0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_feat_df = df_comb.iloc[:,indices[:15]] \n",
    "imp_feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting X and y\n",
    "\n",
    "y_cols = 'Y'\n",
    "\n",
    "X, y = imp_feat_df.values, df_comb[y_cols].values \n",
    "y = y.astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "rf_model = RandomForestClassifier(criterion='gini', random_state=0, n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=-1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974335811734186"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred=y_pred, y_true=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can ve seen above, there seems to be quite a little difference in the performance of the random forests (~61% vs 60% accuracy) by using the 15 most important features vs. using all of them. If the performance hit can be deemed acceptable, modelling with fewer features can greatly improve the training/modelling time of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
