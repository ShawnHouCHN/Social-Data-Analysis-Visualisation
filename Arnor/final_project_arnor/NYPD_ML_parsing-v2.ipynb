{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geoplotlib as gp\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder as LE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading in the data and dropping missing values from locations of the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NYPD_Motor_Vehicle_Collisions_reduced_data.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop NaN values from Lat and Lon\n",
    "df = df.dropna(subset=['LATITUDE','LONGITUDE', 'ON STREET NAME', 'BOROUGH'])\n",
    "\n",
    "# Drop empty strings from street names\n",
    "df = df[~df['ON STREET NAME'].str.contains('^\\s+$')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the most relevant subset of the data, the accidents that occur at locations more than one standard deviation from the mean are discarded. Prior to this the LAT and LON were visualised and seemed somewhat normal distributed, which means that standard deviation is applicable in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(656094, 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(np.abs(stats.zscore(df[['LATITUDE','LONGITUDE']])) < 1).all(axis=1)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick preview of the data shows us how the current dataframe looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>NUMBER OF PERSONS INJURED</th>\n",
       "      <th>NUMBER OF PERSONS KILLED</th>\n",
       "      <th>...</th>\n",
       "      <th>Maximum Humidity</th>\n",
       "      <th>Mean Temperature</th>\n",
       "      <th>Min Temperature</th>\n",
       "      <th>Minimum Humidity</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Sea Level Pressure</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Snow Depth</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Wind Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017/03/28</td>\n",
       "      <td>0:00</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11218</td>\n",
       "      <td>40.654080</td>\n",
       "      <td>-73.977610</td>\n",
       "      <td>(40.65408, -73.97761)</td>\n",
       "      <td>18 STREET</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017/03/28</td>\n",
       "      <td>0:00</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>11429</td>\n",
       "      <td>40.713715</td>\n",
       "      <td>-73.731440</td>\n",
       "      <td>(40.713715, -73.73144)</td>\n",
       "      <td>222 STREET</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017/03/28</td>\n",
       "      <td>0:01</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11234</td>\n",
       "      <td>40.622400</td>\n",
       "      <td>-73.936646</td>\n",
       "      <td>(40.6224, -73.936646)</td>\n",
       "      <td>FLATBUSH AVENUE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017/03/28</td>\n",
       "      <td>10:02</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11230</td>\n",
       "      <td>40.627130</td>\n",
       "      <td>-73.975280</td>\n",
       "      <td>(40.62713, -73.97528)</td>\n",
       "      <td>ELMWOOD AVENUE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017/03/28</td>\n",
       "      <td>10:16</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10455</td>\n",
       "      <td>40.814460</td>\n",
       "      <td>-73.896860</td>\n",
       "      <td>(40.81446, -73.89686)</td>\n",
       "      <td>BRUCKNER BOULEVARD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   TIME   BOROUGH ZIP CODE   LATITUDE  LONGITUDE  \\\n",
       "1   2017/03/28   0:00  BROOKLYN    11218  40.654080 -73.977610   \n",
       "4   2017/03/28   0:00    QUEENS    11429  40.713715 -73.731440   \n",
       "7   2017/03/28   0:01  BROOKLYN    11234  40.622400 -73.936646   \n",
       "16  2017/03/28  10:02  BROOKLYN    11230  40.627130 -73.975280   \n",
       "21  2017/03/28  10:16     BRONX    10455  40.814460 -73.896860   \n",
       "\n",
       "                  LOCATION                    ON STREET NAME  \\\n",
       "1    (40.65408, -73.97761)  18 STREET                          \n",
       "4   (40.713715, -73.73144)  222 STREET                         \n",
       "7    (40.6224, -73.936646)  FLATBUSH AVENUE                    \n",
       "16   (40.62713, -73.97528)  ELMWOOD AVENUE                     \n",
       "21   (40.81446, -73.89686)  BRUCKNER BOULEVARD                 \n",
       "\n",
       "    NUMBER OF PERSONS INJURED  NUMBER OF PERSONS KILLED    ...      \\\n",
       "1                           0                         0    ...       \n",
       "4                           0                         0    ...       \n",
       "7                           0                         0    ...       \n",
       "16                          1                         0    ...       \n",
       "21                          0                         0    ...       \n",
       "\n",
       "    Maximum Humidity  Mean Temperature  Min Temperature  Minimum Humidity  \\\n",
       "1                100                 7                5                89   \n",
       "4                100                 7                5                89   \n",
       "7                100                 7                5                89   \n",
       "16               100                 7                5                89   \n",
       "21               100                 7                5                89   \n",
       "\n",
       "    Precipitation  Sea Level Pressure  Snow Snow Depth  Visibility Wind Speed  \n",
       "1           18.29              1014.0  0.00       0.00         7.0          9  \n",
       "4           18.29              1014.0  0.00       0.00         7.0          9  \n",
       "7           18.29              1014.0  0.00       0.00         7.0          9  \n",
       "16          18.29              1014.0  0.00       0.00         7.0          9  \n",
       "21          18.29              1014.0  0.00       0.00         7.0          9  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed limit parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional feature added to the data is speed limit of each street. The speed limit files were downloaded from http://www.nyc.gov/html/dot/html/about/vz_datafeeds.shtml. A dictionary is made (`speed_dict`) that will be composed of 5 keys and their valeus. Each key is a borough (e.g. Bronx) where the value is another dictionary of street:speed_limit for the streets in that borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_dict = {}\n",
    "files = ['speed_limit_bronx.json', 'speed_limit_brooklyn.json', 'speed_limit_manhattan.json', 'speed_limit_queens.json', 'speed_limit_statenisland.json']\n",
    "\n",
    "# go through all boroguh files\n",
    "for file in files:\n",
    "    temp_df = pd.read_json('speed_limits/' + file) # make a temporary dataframe\n",
    "    temp_df.drop('type', inplace=True, axis=1) # drop the type column\n",
    "    \n",
    "    name = file.split('_')[-1].split('.')[0] # grab the borough name from the filename\n",
    "    \n",
    "    speed_dict[name] = {} # add the borough name to the speed dictionary\n",
    "\n",
    "    # go through each street:speed_limit pair and add them to the current key in the dictionary\n",
    "    for index, row in temp_df.iterrows():\n",
    "        speed_dict[name][temp_df.iloc[index, 0]['properties']['street']] = temp_df.iloc[index, 0]['properties']['postvz_sl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dictionary, a new empty column is added to the dataframe to hold the speed limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['street_SL'] = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go through all collisions and add speed limit on the street that they happened at to a new list (`speeds`), which then gets added as the (`street_SL`) column of our main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boroughs = list(df['BOROUGH'].copy()) # list of all boroughs in the df\n",
    "streets = list(df['ON STREET NAME'].copy()) # list of all street names in the df \n",
    "speeds = [] # a list of all speed limits\n",
    "\n",
    "# cast to lowercase\n",
    "boroughs = [x.lower().replace(\" \", \"\") for x in boroughs]\n",
    "streets = [x.strip() for x in streets]\n",
    "\n",
    "# for each collision\n",
    "for i in range(len(boroughs)):\n",
    "    \n",
    "    # try grabbing the relevant speed limit and add to the speeds list\n",
    "    try:\n",
    "        speed = speed_dict[boroughs[i]][streets[i]]\n",
    "        speeds.append(speed)\n",
    "    \n",
    "    # if we can't find it, add NaN\n",
    "    except KeyError:\n",
    "        speeds.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to a numpy array and add to the dataframe\n",
    "speeds = np.array(speeds)\n",
    "df['street_SL'] = speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally, drop streets that no speed limit was found for\n",
    "df = df.dropna(subset=['street_SL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time is modeled as hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split at the ':' and grab the first value.\n",
    "df['TIME'] = df['TIME'].apply(lambda x: int(x.split(':')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new column for months only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split at '/' and grab the second value\n",
    "df['Month'] = df['Date'].apply(lambda x: int(x.split('/')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use label encoder to numerically encode boroughs in NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BR_dummies = pd.get_dummies(df['BOROUGH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the one-hot encoded columns to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_BR_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, vehicle types will be encoded with one hot as well. First we will replace NaN values with 'OTHER'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']] = df[['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']].fillna('OTHER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the encoding is done, vehicles are grouped into 4 classes depending on their size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_dict = {\n",
    "'two_wheeler': ['BICYCLE', 'PEDICAB', 'SCOOTER', 'MOTORCYCLE'],\n",
    "'small': ['PASSENGER VEHICLE', 'TAXI'],\n",
    "'medium': ['AMBULANCE', 'SPORT UTILITY / STATION WAGON', 'PICK-UP TRUCK', 'SMALL COM VEH(4 TIRES) ', 'LIVERY VEHICLE', 'VAN'],\n",
    "'large': ['BUS', 'FIRE TRUCK', 'LARGE COM VEH(6 OR MORE TIRES)'],\n",
    "'other': ['OTHER', 'UNKNOWN']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And replace those values in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_repl = df[['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']].copy()\n",
    "\n",
    "for new_name in name_dict:\n",
    "    \n",
    "    for old_name in name_dict[new_name]:\n",
    "    \n",
    "        df_repl = df_repl[['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']].replace(to_replace=old_name, value=new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply one-hot endcoding to the vehicle type as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VT_dummies = pd.get_dummies(df_repl[['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']], prefix=['VTC1', 'VTC2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we join that to the dataframe as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_VT_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we do one-hot encoding for events. First we start by replacing NaN with Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Events'] = df['Events'].fillna('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy dataframe to be added to the main dataframe later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EV_dummies = pd.DataFrame(columns=['Rain', 'Snow', 'Fog', 'Other'], index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the dummy dataframe with empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df_EV_dummies.columns:\n",
    "    df_EV_dummies[column] = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go through the 'Events' column in the main dataframe (that contain values such as 'Fog\\n\\t,\\nRain\\n\\t,\\nSnow', split them and add corresponding '1's to the dummy dataframe for each event that happened during a given collision. For example, an 'Events' cell corresponding to 'Fog\\n\\t,\\nRain\\n\\t,\\nSnow' in the main dataframe would end up being\n",
    "\n",
    "Rain | Snow | Fog | Other\n",
    "--- | --- | --- |---\n",
    "1 | 1 | 1 | 0\n",
    "\n",
    "in the dummy events dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all rows in the dataframe\n",
    "for index, row in enumerate(df['Events']):\n",
    "    \n",
    "    # for each event in the cell\n",
    "    for event in row.split(','):\n",
    "        \n",
    "        # remove whitespace\n",
    "        event = event.strip()\n",
    "        \n",
    "        # for each possible event\n",
    "        for c_idx, column in enumerate(['Rain', 'Snow', 'Fog', 'Other']):\n",
    "            \n",
    "            # if it's found, fill the corresponding column in the dummies df\n",
    "            if event == column:\n",
    "                df_EV_dummies.iloc[index, c_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns because of same names in the main df, and remove the Other column as it does not provide valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EV_dummies.columns = ['Rain_EV', 'Snow_EV', 'Fog_EV', 'Other_EV']\n",
    "df_EV_dummies = df_EV_dummies.drop('Other_EV', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join the event dummy and main dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.join(df_EV_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, 'CONTRIBUTING FACTOR VEHICLE 1' was also modelled and encoded with one-hot encoding. However, not nearly all of the collisions have a contributing factor value and dropping them out significantly reduced the size of the data. After doing some ML modelling and performance tests with and without the 'CONTRIBUTING FACTOR VEHICLE 1', it was decided to not include the them as one-hot encoded values as the tradeoff is loss of data was not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some strange values in some of the rows still that should be numeric - for example in the 'Snow' column there are some rows with 'T' instead of snow depth. This also causes that column to be encoded as strings instead of numbers.\n",
    "\n",
    "Therefore we need to cast the cells to numbers where we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.00', 582842),\n",
       " ('T', 27057),\n",
       " ('1.02', 2204),\n",
       " ('0.25', 1846),\n",
       " ('1.27', 1730)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['Snow']).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those that are encoded as strings:\n",
    "\n",
    "- Max Gust Speed\n",
    "- Max Wind Speed\n",
    "- Precipitation\n",
    "- Snow\n",
    "- Snow Depth\n",
    "- Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cast_to_float(x):\n",
    "    \n",
    "    try:\n",
    "        return float(x)\n",
    "        \n",
    "    except Exception as error:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Max Gust Speed', 'Max Wind Speed', 'Precipitation', 'Snow', 'Snow Depth', 'Wind Speed']:\n",
    "    \n",
    "    df[column] = df[column].apply(lambda x: cast_to_float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select the relevant columns for the actual ML modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml = df.drop(['Date', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE',\n",
    "       'LOCATION', 'ON STREET NAME', 'CONTRIBUTING FACTOR VEHICLE 1',\n",
    "       'CONTRIBUTING FACTOR VEHICLE 2', 'UNIQUE KEY', 'VEHICLE TYPE CODE 1',\n",
    "       'VEHICLE TYPE CODE 2', 'Events', 'Dew Point'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can select those rows that only have numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df_ml[df_ml.applymap(np.isreal).all(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add our y (any person injured or killed) to our dataframe. First we add an empty column to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml['Y'] = np.zeros(df_ml.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look at the sum of people injured / killed in all the collisions. We sum over all the columns that keep track of casualties, and if are any casualties that is a positive observation (y = 1) and if there are non it is a negative observation (y = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ml[['NUMBER OF PERSONS INJURED',\n",
    "       'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',\n",
    "       'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',\n",
    "       'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED',\n",
    "       'NUMBER OF MOTORIST KILLED']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [1 if x > 0 else 0 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml['Y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then of course we have to drop the columns that directly measure if people were killed or injured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml = df_ml.drop(['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED', \n",
    "                    'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "                   'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this parsed ML dataframe to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml.to_csv('NYPD_ML_df_wY.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating ML models it is important to have balanced classes, as it can otherwise lead us to believe our model is doing very well when in fact it is just predicting all observations to be in a majority class. We start by reading in the dataframe we had saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml = pd.read_csv('NYPD_ML_df_wY.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some NaN values in the dataset - we drop them before balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ml = df_ml.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the counts of our Y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 453018, 1: 107931})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_ml['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select our positive and negative classes from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_all = df_ml[df_ml['Y'] == 1]\n",
    "neg_all = df_ml[df_ml['Y'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the positive class is smaller than the negative class, we sample randomly from the negative class (without replacement), with the number of samples equaling the total amount of positive samples we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pos = pos_all.shape[0]\n",
    "neg_sub = neg_all.sample(no_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we concatenate the equally sized positive and negative samples in a final dataframe, and save it as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comb = pd.concat([pos_all, neg_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comb.to_csv('NYPD_ML_df_wY_balanced.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
