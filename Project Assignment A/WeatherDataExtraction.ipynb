{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "# IPython global cell magic\n",
    "%reset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import bs4  # HTML parser\n",
    "from collections import Counter, OrderedDict, defaultdict  # counting elements and ordering keys in dictionaries\n",
    "import cStringIO # use text in memory\n",
    "import csv # read csv data\n",
    "import datetime  # handle date objects\n",
    "from __future__ import division  # all numbers are float\n",
    "import gc  # garbage collector\n",
    "import geoplotlib  # plot points on tiled maps\n",
    "from geoplotlib.utils import BoundingBox\n",
    "import geopy  # get geo location according to addresses\n",
    "from geopy.exc import GeocoderServiceError\n",
    "#from infomap import infomap  # python infomap algorithm (store in NB dir)\n",
    "import itertools  # iterators for efficient looping\n",
    "import json  # JSON parser\n",
    "import math  # math operations\n",
    "from matplotlib import pyplot as plt  # plotting figures\n",
    "plt.style.use('ggplot') # plotstyle\n",
    "import Levenshtein # computing string edit distances and similarities\n",
    "#import mwparserfromhell  # parse MediaWiki syntax: https://github.com/earwig/mwparserfromhell\n",
    "#from nameparser import HumanName  # parse a human name\n",
    "import networkx as nx  # networks creation library\n",
    "import nltk  # natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np  # powerful calculation library\n",
    "import operator  # efficient operator functions\n",
    "import os  # operating system operations, e.g.: with files and folders\n",
    "import pandas as pd  # use easy-to-use data frames for data analysis\n",
    "import pickle  # python data structures as files\n",
    "from pprint import pprint  # print data structures prettier\n",
    "import random as rand  # pick thing at random\n",
    "import re  # regex\n",
    "import requests  # request URL content\n",
    "from scipy.misc import imread # mask for wordcloud\n",
    "import sys  # system operations\n",
    "import time  # sleep timer\n",
    "from tqdm import tqdm_notebook  # make a nice progressbar\n",
    "import tweepy # twitter API abstraction\n",
    "import urllib  # handle special URL chars\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years=[\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"] \n",
    "\n",
    "#year=\"2012\"\n",
    "#start_date = '2012/1/1'\n",
    "#end_date = '2012/12/31'\n",
    "#dates_2012=pd.date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "Temperature\n",
      " \n",
      "Degree Days\n",
      " \n",
      "Moisture\n",
      " \n",
      "Precipitation\n",
      " \n",
      "Snow\n",
      " \n",
      "Sea Level Pressure\n",
      " \n",
      "Wind\n",
      " \n"
     ]
    }
   ],
   "source": [
    "html = \"https://www.wunderground.com/history/airport/KNYC/2012/1/1/DailyHistory.html?req_city=New+York&req_state=NY&req_statename=&reqdb.zip=10001&reqdb.magic=11&reqdb.wmo=99999\"\n",
    "r  = requests.get(html)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data)\n",
    "hist_table=soup.find(\"table\", id=\"historyTable\")\n",
    "hist_categories=hist_table.find_all(\"td\", class_=\"history-table-grey-header\")\n",
    "print \"********************************************************\"\n",
    "for hist_cate in hist_categories:\n",
    "    print hist_cate.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03', '2012-01-04',\n",
      "               '2012-01-05', '2012-01-06', '2012-01-07', '2012-01-08',\n",
      "               '2012-01-09', '2012-01-10',\n",
      "               ...\n",
      "               '2012-12-22', '2012-12-23', '2012-12-24', '2012-12-25',\n",
      "               '2012-12-26', '2012-12-27', '2012-12-28', '2012-12-29',\n",
      "               '2012-12-30', '2012-12-31'],\n",
      "              dtype='datetime64[ns]', length=366, freq='D')\n",
      "DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
      "               '2013-01-05', '2013-01-06', '2013-01-07', '2013-01-08',\n",
      "               '2013-01-09', '2013-01-10',\n",
      "               ...\n",
      "               '2013-12-22', '2013-12-23', '2013-12-24', '2013-12-25',\n",
      "               '2013-12-26', '2013-12-27', '2013-12-28', '2013-12-29',\n",
      "               '2013-12-30', '2013-12-31'],\n",
      "              dtype='datetime64[ns]', length=365, freq='D')\n",
      "DatetimeIndex(['2014-01-01', '2014-01-02', '2014-01-03', '2014-01-04',\n",
      "               '2014-01-05', '2014-01-06', '2014-01-07', '2014-01-08',\n",
      "               '2014-01-09', '2014-01-10',\n",
      "               ...\n",
      "               '2014-12-22', '2014-12-23', '2014-12-24', '2014-12-25',\n",
      "               '2014-12-26', '2014-12-27', '2014-12-28', '2014-12-29',\n",
      "               '2014-12-30', '2014-12-31'],\n",
      "              dtype='datetime64[ns]', length=365, freq='D')\n",
      "DatetimeIndex(['2015-01-01', '2015-01-02', '2015-01-03', '2015-01-04',\n",
      "               '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08',\n",
      "               '2015-01-09', '2015-01-10',\n",
      "               ...\n",
      "               '2015-12-22', '2015-12-23', '2015-12-24', '2015-12-25',\n",
      "               '2015-12-26', '2015-12-27', '2015-12-28', '2015-12-29',\n",
      "               '2015-12-30', '2015-12-31'],\n",
      "              dtype='datetime64[ns]', length=365, freq='D')\n",
      "DatetimeIndex(['2016-01-01', '2016-01-02', '2016-01-03', '2016-01-04',\n",
      "               '2016-01-05', '2016-01-06', '2016-01-07', '2016-01-08',\n",
      "               '2016-01-09', '2016-01-10',\n",
      "               ...\n",
      "               '2016-12-22', '2016-12-23', '2016-12-24', '2016-12-25',\n",
      "               '2016-12-26', '2016-12-27', '2016-12-28', '2016-12-29',\n",
      "               '2016-12-30', '2016-12-31'],\n",
      "              dtype='datetime64[ns]', length=366, freq='D')\n",
      "DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04',\n",
      "               '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08',\n",
      "               '2017-01-09', '2017-01-10',\n",
      "               ...\n",
      "               '2017-12-22', '2017-12-23', '2017-12-24', '2017-12-25',\n",
      "               '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29',\n",
      "               '2017-12-30', '2017-12-31'],\n",
      "              dtype='datetime64[ns]', length=365, freq='D')\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    start_date = year+\"/1/1\"\n",
    "    end_date = year+\"/12/31\"\n",
    "    dates=pd.date_range(start_date, end_date)\n",
    "    print dates\n",
    "    list_=[]\n",
    "    for date_ in dates:\n",
    "        item_={}\n",
    "        datelabel_=date_.strftime('%Y/%m/%d')\n",
    "        item_[u\"Date\"]=datelabel_\n",
    "\n",
    "        datestr_=date_.strftime('%Y/%m/%d').replace('/0', '/')\n",
    "        html = \"https://www.wunderground.com/history/airport/KNYC/\"+datestr_+\"/DailyHistory.html?req_city=New+York&req_state=NY&req_statename=&reqdb.zip=10001&reqdb.magic=11&reqdb.wmo=99999\"\n",
    "        r  = requests.get(html)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data)\n",
    "        hist_table=soup.find(\"table\", id=\"historyTable\")\n",
    "        sub_categories=hist_table.find_all(\"td\", class_=\"indent\")\n",
    "        for sub_cate in sub_categories:\n",
    "            subcat_title=sub_cate.get_text()\n",
    "            hist_actual=sub_cate.findNext(\"td\")\n",
    "            value=hist_actual.find(\"span\",class_=\"wx-value\")\n",
    "            if value:\n",
    "                hist_value = value.get_text()\n",
    "            else :\n",
    "                hist_value = hist_actual.get_text().strip()\n",
    "            item_[subcat_title]=hist_value\n",
    "        list_.append(item_)\n",
    "    \n",
    "    pd_=pd.DataFrame(list_)\n",
    "    pd_.to_csv(year+\"_weather.csv\",sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
